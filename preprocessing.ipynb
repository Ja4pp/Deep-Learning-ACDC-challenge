{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e146f34-847a-4370-93dd-6e6efbaad913",
   "metadata": {},
   "source": [
    "# Preprocessing code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7fe633c-5b3f-446b-bd9f-77fbb588b9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from skimage import transform\n",
    "import nibabel as nib\n",
    "import tqdm\n",
    "from PIL import Image\n",
    "from skimage.restoration import denoise_tv_chambolle\n",
    "from statistics import mean, median\n",
    "\n",
    "\n",
    "def normalize_img(img):\n",
    "    # Warning for when dividing NaN value??\n",
    "    norm_img = np.divide(img,np.max(img))\n",
    "    return norm_img\n",
    "\n",
    "\n",
    "def crop_pad_resize(image, nx, ny):\n",
    "    '''\n",
    "    Code from Christian F. Baumgartner and Lisa M. Koch from\n",
    "    \"An Exploration of 2D and 3D Deep Learning Techniques for\n",
    "    Cardiac MR Image Segmentation\" (2017).\n",
    "    Comments by us.\n",
    "    '''\n",
    "    x, y = image.shape\n",
    "\n",
    "    # difference in nr of pixels (divide by 2 since we have 2 sides)\n",
    "    x_s = (x - nx) // 2\n",
    "    y_s = (y - ny) // 2\n",
    "    x_c = (nx - x) // 2\n",
    "    y_c = (ny - y) // 2\n",
    "\n",
    "    if x > nx and y > ny:\n",
    "        # if image is larger in both dimensions cut a slice\n",
    "        slice_cropped = image[x_s:x_s + nx, y_s:y_s + ny]\n",
    "\n",
    "    else:\n",
    "        # if one dim is smaller fill that side up with 0's\n",
    "        slice_cropped = np.zeros((nx, ny))\n",
    "\n",
    "        if x <= nx and y > ny:\n",
    "            # fill up x direction with 0's, cut in y direction\n",
    "            slice_cropped[x_c:x_c + x, :] = image[:, y_s:y_s + ny]\n",
    "        elif x > nx and y <= ny:\n",
    "            # fill up y direction with 0's, cut in x direction\n",
    "            slice_cropped[:, y_c:y_c + y] = image[x_s:x_s + nx, :]\n",
    "        else:\n",
    "            # if dimensions are as desired, keep the original slice\n",
    "            slice_cropped[x_c:x_c + x, y_c:y_c + y] = image[:, :]\n",
    "\n",
    "    return slice_cropped\n",
    "\n",
    "\n",
    "def preprocess(input_folder, target_resolution, target_size, denoise=False, alphaTV=0.2):\n",
    "    '''\n",
    "    This function preprocesses ACDC data. It crops all images to the same size,\n",
    "    transforms everything to the same resolution and normalizes the images.\n",
    "    It automatically makes the folder where preprocessed data is written to,\n",
    "    in the same format as the ACDC data is given. The images are in PNG-format.\n",
    "    If wanted, it can denoise the data as well. It will put this in a different folder.\n",
    "    If you want to have denoised and non-denoised data, run the function twice with denoise\n",
    "    on False and True.\n",
    "    The function outputs the scale vectors and original image sizes so we can transform the masks\n",
    "    back to the original input format.\n",
    "    \n",
    "    input_folder: the folder where raw ACDC data is located.\n",
    "    target_resolution: desired resolution, should be a tuple with 2 items (x- and y-dimensions).\n",
    "    target_size: desired size. Should be a tuple wiht 2 items (x- and y-dimensions).\n",
    "    alphaTV: parameter used in the TV denoising.\n",
    "    '''\n",
    "    nx, ny = target_size\n",
    "    data_folder = input_folder\n",
    "    # i = 0  # iterator for saving original resolution and size\n",
    "    scale_vectors = [[0, 0]]\n",
    "    original_image_size = [[0, 0]]\n",
    "    \n",
    "    if denoise:\n",
    "        foldername = 'preprocessed_denoised'\n",
    "    else:\n",
    "        foldername = 'preprocessed'\n",
    "    \n",
    "    if not os.path.exists(foldername):\n",
    "        os.mkdir(foldername)\n",
    "    else:\n",
    "        print(foldername+' folder already exists. Continuing regardless.')\n",
    "        \n",
    "    # print(target_resolution)\n",
    "    \n",
    "    # Loop over train and test folders\n",
    "    heightImage = []\n",
    "    widthImage = []\n",
    "    voxelImage = []\n",
    "    depthImage = []\n",
    "    groupNOR = []\n",
    "    groupMINF = []\n",
    "    groupDCM = []\n",
    "    groupHCM = []\n",
    "    groupRV = []\n",
    "    maxInt = []\n",
    "    minInt = []\n",
    "    meanInt = []\n",
    "    for train_test in ['training', 'testing']:\n",
    "\n",
    "        input_folder = os.path.join(data_folder, train_test)\n",
    "        len_inp = len(input_folder)+1\n",
    "        \n",
    "#         if os.path.exists(os.path.join(input_folder+'/'+train_test, '.ipynb_checkpoints')):\n",
    "#             n = len(os.listdir(input_folder)) - 1\n",
    "#         else:\n",
    "#             n = len(os.listdir(input_folder))\n",
    "            \n",
    "#         original_pixel_size = np.zeros((n, 2))\n",
    "#         original_image_size = np.zeros((n, 2))\n",
    "        \n",
    "        # Make train and test folders in preprocessed folder\n",
    "        if not os.path.exists(os.path.join(foldername+'/', train_test)):\n",
    "            os.mkdir(os.path.join(foldername+'/', train_test))\n",
    "        else:\n",
    "            print('T'+train_test[1:]+' folder already exists. Continuing regardless.')\n",
    "        \n",
    "        # Loop over patient folders\n",
    "        for folder in os.listdir(input_folder):\n",
    "            \n",
    "            if folder != '.ipynb_checkpoints':  # Sometimes trouble with automatically made files\n",
    "\n",
    "                folder_path = os.path.join(input_folder, folder)\n",
    "                \n",
    "                # Make patient folders in preprocessed folder\n",
    "                if not os.path.exists(os.path.join(foldername+'/'+train_test, folder_path[len_inp:])):\n",
    "                    os.mkdir(os.path.join(foldername+'/'+train_test, folder_path[len_inp:]))\n",
    "                else:\n",
    "                    print('Folder for '+folder_path[len_inp:]+' already exists. Continuing regardless.')\n",
    "                \n",
    "                if os.path.exists(foldername+'/'+train_test+'/'+folder_path[len_inp:]+'/.ipynb_checkpoints'):\n",
    "                    os.rmdir(foldername+'/'+train_test+'/'+folder_path[len_inp:]+'/.ipynb_checkpoints')\n",
    "                    \n",
    "                lst = os.listdir(foldername+'/'+train_test+'/'+folder_path[len_inp:])\n",
    "                \n",
    "                if len(lst) == 0:  # Only create files if the designated folder is empty\n",
    "                    \n",
    "                    for file in glob.glob(os.path.join(folder_path, 'patient???_frame??.nii.gz')):\n",
    "\n",
    "                        # Save information about patient\n",
    "                        with open(os.path.join(folder_path, 'Info.cfg')) as f:\n",
    "                            lines = f.readlines()\n",
    "\n",
    "                        ED = int(lines[0].strip()[-2:])\n",
    "                        ES = int(lines[1].strip()[-2:])\n",
    "                        Group = lines[2].strip().split(\":\")\n",
    "                        Group2 = lines[5].strip().split(\":\")\n",
    "                        Group3 = float(Group2[1].strip())\n",
    "                        # print(Group)\n",
    "                       \n",
    "                        \n",
    "                        # Split file name\n",
    "                        file_base = file.split('.nii.gz')[0]\n",
    "                        file_mask = file_base + '_gt.nii.gz'\n",
    "\n",
    "                        # Load data from .nii.gz files\n",
    "                        img_nii = nib.load(file)\n",
    "                        img_dat = img_nii.get_fdata()\n",
    "                        \n",
    "                        heightImage.append(img_dat.shape[0])\n",
    "                        widthImage.append(img_dat.shape[1])\n",
    "                        depthImage.append(img_dat.shape[2])\n",
    "                        # print(np.amax(np.uint8(img_dat)))\n",
    "                        # print(np.amin(np.uint8(img_dat)))\n",
    "                        maxInt.append(np.amax(np.uint8(img_dat)))\n",
    "                        minInt.append(np.amin(np.uint8(img_dat)))\n",
    "                        meanInt.append(np.mean(np.uint8(img_dat)))\n",
    "                                      \n",
    "                        mask_nii = nib.load(file_mask)\n",
    "                        mask_dat = mask_nii.get_fdata()\n",
    "\n",
    "                        img = img_nii.get_fdata()\n",
    "                        mask = mask_nii.get_fdata()\n",
    "\n",
    "                        pixel_size = img_nii.header.get_zooms()\n",
    "                        \n",
    "                        sx, sy, sz  = img_nii.header.get_zooms()\n",
    "                        volume_vox = target_resolution[0]*target_resolution[1]*sz\n",
    "                        voxelImage.append(volume_vox)\n",
    "                        \n",
    "                        \n",
    "                        # print(str(volume_vox) + ' mm^3')\n",
    "                        # print(str(sx) + ' x')\n",
    "                        # print(str(sy) + ' y')\n",
    "                        # Save original pixel and image size before transforming\n",
    "                        # original_pixel_size = np.append(original_pixel_size, [[pixel_size[0], pixel_size[1]]], axis = 0)\n",
    "                        original_image_size = np.append(original_image_size, [[img.shape[0], img.shape[1]]], axis=0)\n",
    "                            \n",
    "                        # Make vector to make all images have the same resolution\n",
    "                        scale_vector = [pixel_size[0] / target_resolution[0], pixel_size[1] / target_resolution[1]] \n",
    "                        scale_vectors = np.append(scale_vectors, [scale_vector], axis=0)\n",
    "                        \n",
    "                        for zz in tqdm.tqdm(range(img.shape[2])):\n",
    "                            \n",
    "                            if(Group[1] == ' NOR'):\n",
    "                                groupNOR.append(Group3)\n",
    "                            elif(Group[1] == ' MINF'):\n",
    "                                groupMINF.append(Group3)\n",
    "                            elif(Group[1] == ' DCM'):\n",
    "                                groupDCM.append(Group3)    \n",
    "                            elif(Group[1] == ' HCM'):\n",
    "                                groupHCM.append(Group3)\n",
    "                            elif(Group[1] == ' RV'):\n",
    "                                groupRV.append(Group3)\n",
    "\n",
    "                            # Normalize, rescale and crop the image and  mask\n",
    "\n",
    "                            slice_img = np.squeeze(img[:, :, zz])\n",
    "                            slice_img = normalize_img(np.squeeze(img[:, :, zz]))\n",
    "                            img_rescaled = transform.rescale(slice_img,\n",
    "                                                             scale_vector,\n",
    "                                                             order=1,\n",
    "                                                             preserve_range=True,\n",
    "                                                             mode='constant')\n",
    "\n",
    "                            slice_mask = np.squeeze(mask[:, :, zz])\n",
    "                            \n",
    "                            # slice_mask = normalize_img(np.squeeze(mask[:, :, zz]))\n",
    "                            mask_rescaled = transform.rescale(slice_mask,\n",
    "                                                              scale_vector,\n",
    "                                                              order=0,\n",
    "                                                              preserve_range=True,\n",
    "                                                              mode='constant')\n",
    "\n",
    "                            img_cropped = crop_pad_resize(img_rescaled, nx, ny)\n",
    "                            mask_cropped = crop_pad_resize(mask_rescaled, nx, ny)\n",
    "                            \n",
    "                            if denoise:\n",
    "                                img_cropped = denoise_tv_chambolle(img_cropped, eps=1e-6, weight=alphaTV, max_num_iter=1000)\n",
    "\n",
    "                            # Save images in PNG format\n",
    "                            if 'frame{:02}'.format(ED) in file:\n",
    "                                img_loc = os.path.join(foldername+'/'+train_test, file[len_inp:-7]+'_slice{:01}_ED_'.format(zz)+ str(volume_vox) +'.png')\n",
    "                                img_fin = Image.fromarray(np.uint8(255 * img_cropped),mode=\"L\")\n",
    "                                img_fin.save(img_loc, format='PNG')\n",
    "\n",
    "                                mask_loc = os.path.join(foldername+'/'+train_test, file[len_inp:-7]+'_slice{:01}_ED_'.format(zz)+ str(volume_vox) + '_gt.png')\n",
    "                                mask_fin = Image.fromarray(np.uint8(mask_cropped), mode=\"L\")\n",
    "                                mask_fin.save(mask_loc, format='PNG')\n",
    "                            else:\n",
    "                                img_loc = os.path.join(foldername+'/'+train_test, file[len_inp:-7]+'_slice{:01}_ES_'.format(zz)+  str(volume_vox) + '.png')\n",
    "                                img_fin = Image.fromarray(np.uint8(255 * img_cropped), mode=\"L\")\n",
    "                                img_fin.save(img_loc, format='PNG')\n",
    "\n",
    "                                mask_loc = os.path.join(foldername+'/'+train_test, file[len_inp:-7]+'_slice{:01}_ES_'.format(zz)+ str(volume_vox) + '_gt.png')\n",
    "                                mask_fin = Image.fromarray(np.uint8(mask_cropped),mode=\"L\")\n",
    "                                mask_fin.save(mask_loc, format='PNG')\n",
    "                    \n",
    "                else:\n",
    "                    print('Folder for '+folder_path[len_inp:]+' is not empty. No files were written to this folder.')\n",
    "    \n",
    "    scale_vectors = np.delete(scale_vectors, 0, axis=0)\n",
    "    original_image_size = np.delete(original_image_size, 0, axis=0)\n",
    "    # print(max(depthImage))\n",
    "    # print(min(depthImage))\n",
    "    # # print(mean(depthImage))\n",
    "    # print(max(groupNOR))\n",
    "    # print(min(groupNOR))\n",
    "    # print(mean(groupNOR))\n",
    "    # print(max(groupMINF))\n",
    "    # print(min(groupMINF))\n",
    "    # print(mean(groupMINF))\n",
    "    # print(max(groupDCM))\n",
    "    # print(min(groupDCM))\n",
    "    # print(mean(groupDCM))\n",
    "    # print(max(groupHCM))\n",
    "    # print(min(groupHCM))\n",
    "    # print(mean(groupHCM))\n",
    "    # print(max(groupRV))\n",
    "    # print(min(groupRV))\n",
    "    # print(mean(groupRV))\n",
    "    print(\"Int\")\n",
    "    print(max(maxInt))\n",
    "    print(max(minInt))\n",
    "    print(max(meanInt))\n",
    "\n",
    "    print(min(maxInt))\n",
    "    print(min(minInt))\n",
    "    print(min(meanInt))\n",
    "\n",
    "    print(mean(maxInt))\n",
    "    print(mean(minInt))\n",
    "    print(mean(meanInt))\n",
    "    # print(min(heightImage))\n",
    "    # print(min(widthImage))\n",
    "    # print(min(voxelImage))\n",
    "    # print(mean(heightImage))\n",
    "    # print(mean(widthImage))\n",
    "    # print(mean(voxelImage))\n",
    "    print('Preprocessed Finished!')\n",
    "   \n",
    "    \n",
    "    return scale_vectors, original_image_size\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "456bd615-b312-41a7-84f5-5db03801a938",
   "metadata": {},
   "outputs": [],
   "source": [
    "def secret_preprocess(input_folder, target_resolution, target_size, denoise=False, alphaTV=0.2):\n",
    "    '''\n",
    "    This function preprocesses ACDC data. It crops all images to the same size,\n",
    "    transforms everything to the same resolution and normalizes the images.\n",
    "    It automatically makes the folder where preprocessed data is written to,\n",
    "    in the same format as the ACDC data is given. The images are in PNG-format.\n",
    "    If wanted, it can denoise the data as well. It will put this in a different folder.\n",
    "    If you want to have denoised and non-denoised data, run the function twice with denoise\n",
    "    on False and True.\n",
    "    The function outputs the scale vectors and original image sizes so we can transform the masks\n",
    "    back to the original input format.\n",
    "    \n",
    "    input_folder: the folder where raw ACDC data is located.\n",
    "    target_resolution: desired resolution, should be a tuple with 2 items (x- and y-dimensions).\n",
    "    target_size: desired size. Should be a tuple wiht 2 items (x- and y-dimensions).\n",
    "    alphaTV: parameter used in the TV denoising.\n",
    "    '''\n",
    "    nx, ny = target_size\n",
    "    data_folder = input_folder\n",
    "    # i = 0  # iterator for saving original resolution and size\n",
    "    scale_vectors = [[0, 0]]\n",
    "    original_image_size = [[0, 0]]\n",
    "    \n",
    "    if denoise:\n",
    "        foldername = 'preprocessed_denoised'\n",
    "    else:\n",
    "        foldername = 'secret_preprocessed'\n",
    "    \n",
    "    if not os.path.exists(foldername):\n",
    "        os.mkdir(foldername)\n",
    "    else:\n",
    "        print(foldername+' folder already exists. Continuing regardless.')\n",
    "    \n",
    "    # Loop over train and test folders\n",
    "    train_test = 'secret_test'\n",
    "\n",
    "    input_folder = os.path.join(data_folder, train_test)\n",
    "    len_inp = len(input_folder)+1\n",
    "\n",
    "    # Make train and test folders in preprocessed folder\n",
    "    if not os.path.exists(os.path.join(foldername+'/', train_test)):\n",
    "        os.mkdir(os.path.join(foldername+'/', train_test))\n",
    "    else:\n",
    "        print('T'+train_test[1:]+' folder already exists. Continuing regardless.')\n",
    "\n",
    "    # Loop over patient folders\n",
    "    for i, folder in enumerate(os.listdir(input_folder)):\n",
    "\n",
    "        if folder != '.ipynb_checkpoints':  # Sometimes trouble with automatically made files\n",
    "\n",
    "            folder_path = os.path.join(input_folder, folder)\n",
    "\n",
    "            # Make patient folders in preprocessed folder\n",
    "            if not os.path.exists(os.path.join(foldername+'/'+train_test, folder_path[len_inp:])):\n",
    "                os.mkdir(os.path.join(foldername+'/'+train_test, folder_path[len_inp:]))\n",
    "            else:\n",
    "                print('Folder for '+folder_path[len_inp:]+' already exists. Continuing regardless.')\n",
    "\n",
    "            if os.path.exists(foldername+'/'+train_test+'/'+folder_path[len_inp:]+'/.ipynb_checkpoints'):\n",
    "                os.rmdir(foldername+'/'+train_test+'/'+folder_path[len_inp:]+'/.ipynb_checkpoints')\n",
    "\n",
    "            lst = os.listdir(foldername+'/'+train_test+'/'+folder_path[len_inp:])\n",
    "\n",
    "            if len(lst) == 0:  # Only create files if the designated folder is empty\n",
    "\n",
    "                for file in glob.glob(os.path.join(folder_path, 'patient???_frame??.nii.gz')):\n",
    "                    # Load data from .nii.gz files\n",
    "                    img_nii = nib.load(file)\n",
    "                    img = img_nii.get_fdata()\n",
    "                    \n",
    "                    # Take the slices of the i'th patient (out of the 25)\n",
    "                    img = img[:,:,:,i]\n",
    "                    \n",
    "                    pixel_size = img_nii.header.get_zooms()\n",
    "\n",
    "                    # Save original pixel and image size before transforming\n",
    "                    original_image_size = np.append(original_image_size, [[img.shape[0], img.shape[1]]], axis=0)\n",
    "\n",
    "                    # Make vector to make all images have the same resolution\n",
    "                    scale_vector = [pixel_size[0] / target_resolution[0], pixel_size[1] / target_resolution[1]]\n",
    "                    scale_vectors = np.append(scale_vectors, [scale_vector], axis=0)\n",
    "\n",
    "                    for sliice in tqdm.tqdm(range(img.shape[2])):\n",
    "\n",
    "                        # Normalize, rescale and crop the image and  mask\n",
    "                        slice_img = np.squeeze(img[:, :, sliice])\n",
    "                        slice_img = normalize_img(np.squeeze(img[:, :, sliice]))\n",
    "                        img_rescaled = transform.rescale(slice_img,\n",
    "                                                         scale_vector,\n",
    "                                                         order=1,\n",
    "                                                         preserve_range=True,\n",
    "                                                         mode='constant')\n",
    "\n",
    "                        img_cropped = crop_pad_resize(img_rescaled, nx, ny)\n",
    "\n",
    "                        if denoise:\n",
    "                            img_cropped = denoise_tv_chambolle(img_cropped, eps=1e-6, weight=alphaTV, max_num_iter=1000)\n",
    "\n",
    "                        # Save images in PNG format\n",
    "                        img_loc = os.path.join(foldername+'/'+train_test, file[len_inp:-7]+'_slice{:02}'.format(sliice)+'.png')\n",
    "                        img_fin = Image.fromarray(np.uint8(255 * img_cropped),mode=\"L\")\n",
    "                        img_fin.save(img_loc, format='PNG')\n",
    "\n",
    "            else:\n",
    "                print('Folder for '+folder_path[len_inp:]+' is not empty. No files were written to this folder.')\n",
    "    \n",
    "    scale_vectors = np.delete(scale_vectors, 0, axis=0)\n",
    "    original_image_size = np.delete(original_image_size, 0, axis=0)\n",
    "    \n",
    "    return scale_vectors, original_image_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbca9d3-6919-4562-b86c-dd2e32944bac",
   "metadata": {},
   "source": [
    "#### Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c785ee5-f3a6-4c21-a1bc-ab3776dca597",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 43.02it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 43.46it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 43.78it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 43.41it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 30.27it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 30.96it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 44.98it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 44.53it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 45.03it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 44.35it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 45.00it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 44.44it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 53.13it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 53.60it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 46.46it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 47.43it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 25.68it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 47.26it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 48.24it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 48.05it/s]\n",
      "100%|██████████| 18/18 [00:00<00:00, 52.42it/s]\n",
      "100%|██████████| 18/18 [00:00<00:00, 45.34it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 56.27it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 54.48it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 58.21it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 34.63it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 48.24it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 47.96it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 51.99it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 52.44it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 43.35it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 43.36it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 38.49it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 40.26it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 49.81it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 49.95it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 56.61it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 41.05it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 45.06it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 44.99it/s]\n",
      "100%|██████████| 13/13 [00:00<00:00, 43.39it/s]\n",
      "100%|██████████| 13/13 [00:00<00:00, 41.52it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 52.70it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 52.19it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 43.56it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 43.86it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 50.99it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 32.98it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 64.63it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 64.43it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 72.09it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 71.61it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 62.41it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 61.35it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 61.86it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 60.07it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 74.75it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 74.15it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 53.98it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 51.00it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 63.53it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 63.77it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 67.99it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 50.23it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 50.46it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 49.17it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 35.50it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 48.40it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 48.19it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 47.27it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 55.60it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 54.33it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 49.35it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 39.96it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 16.45it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 49.15it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 51.46it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 50.87it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 46.82it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 46.32it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 50.22it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 48.65it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 39.17it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 39.92it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 52.55it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 52.11it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 34.93it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 39.38it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 50.00it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 44.05it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 44.10it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 43.37it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 45.48it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 45.12it/s]\n",
      "100%|██████████| 14/14 [00:00<00:00, 60.18it/s]\n",
      "100%|██████████| 14/14 [00:00<00:00, 37.60it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 46.16it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 45.47it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 44.27it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 43.86it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 42.15it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 41.40it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 40.60it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 38.48it/s]\n",
      "100%|██████████| 14/14 [00:00<00:00, 45.06it/s]\n",
      "100%|██████████| 14/14 [00:00<00:00, 45.73it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 48.48it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 47.47it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 46.06it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 45.14it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 50.35it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 49.19it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 46.15it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 45.20it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 14.18it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 50.83it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 62.10it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 57.39it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 72.97it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 75.46it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 73.86it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 75.69it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 66.10it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 55.30it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 53.38it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 28.71it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 66.48it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 63.81it/s]\n",
      "100%|██████████| 17/17 [00:00<00:00, 74.23it/s]\n",
      "100%|██████████| 17/17 [00:00<00:00, 47.67it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 69.64it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 67.93it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 75.33it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 33.11it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 49.12it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 63.36it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 63.12it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 61.72it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 69.79it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 68.22it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 74.34it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 75.34it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 48.64it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 61.67it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 67.78it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 68.13it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 70.81it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 70.33it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 64.27it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 63.47it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 76.17it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 74.57it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 34.37it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 72.41it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 77.63it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 37.88it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 72.71it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 71.20it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 72.18it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 73.81it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 67.72it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 67.44it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 69.25it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 68.96it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 63.92it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 57.40it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 64.52it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 61.25it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 75.82it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 72.50it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 74.80it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 72.70it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 66.78it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 67.30it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 69.92it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 71.02it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 74.43it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 42.55it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 60.86it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 60.92it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 73.34it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 72.45it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 79.78it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 79.53it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 72.44it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 70.66it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 76.14it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 52.14it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 49.96it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 52.41it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 68.79it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 68.29it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 58.60it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 57.67it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 61.06it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 60.98it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 70.35it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 68.27it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 35.14it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 64.85it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 63.25it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 61.46it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 70.74it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 72.36it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 70.15it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 71.63it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 63.64it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 62.26it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 54.88it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 52.07it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 60.13it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 59.59it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 62.00it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 62.86it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 73.92it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 70.21it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 72.92it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 73.56it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 79.92it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 21.38it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 73.75it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 45.79it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 54.92it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 54.59it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 70.74it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 72.33it/s]\n",
      "100%|██████████| 17/17 [00:00<00:00, 52.91it/s]\n",
      "100%|██████████| 17/17 [00:00<00:00, 21.73it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 50.90it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 51.86it/s]\n",
      "100%|██████████| 19/19 [00:00<00:00, 57.08it/s]\n",
      "100%|██████████| 19/19 [00:00<00:00, 50.80it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 53.30it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 51.66it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 52.64it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 51.65it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 48.81it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 48.77it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 51.25it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 50.12it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 45.67it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 45.84it/s]\n",
      "100%|██████████| 20/20 [00:00<00:00, 53.06it/s]\n",
      "100%|██████████| 20/20 [00:00<00:00, 51.76it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 27.48it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 40.41it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 52.00it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 50.95it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 51.10it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 49.92it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 48.28it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 47.44it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 50.38it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 51.01it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 34.95it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 50.16it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 55.52it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 54.43it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 52.38it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 52.73it/s]\n",
      "100%|██████████| 21/21 [00:00<00:00, 54.62it/s]\n",
      "100%|██████████| 21/21 [00:00<00:00, 21.27it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 45.20it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 44.78it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 45.57it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 45.34it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 38.51it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 38.32it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 53.44it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 51.31it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 36.52it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 39.46it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 42.44it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 42.67it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 48.63it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 48.23it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 47.49it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 46.56it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 47.39it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 18.58it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 55.68it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 55.06it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 50.11it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 49.61it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 45.24it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 44.68it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 43.04it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 42.07it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 52.75it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 49.86it/s]\n",
      "100%|██████████| 17/17 [00:00<00:00, 54.89it/s]\n",
      "100%|██████████| 17/17 [00:00<00:00, 36.97it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 57.31it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 56.27it/s]\n",
      "100%|██████████| 17/17 [00:00<00:00, 51.63it/s]\n",
      "100%|██████████| 17/17 [00:00<00:00, 29.89it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 13.40it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 52.94it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 43.83it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 43.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int\n",
      "255\n",
      "32\n",
      "96.39805385044643\n",
      "175\n",
      "0\n",
      "38.8757357893319\n",
      "251\n",
      "10\n",
      "56.65650121521835\n",
      "Preprocessed Finished!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "target_resolution = (1.36719, 1.36719)\n",
    "target_size = (256, 256)\n",
    "data_path = './database/'\n",
    "\n",
    "scale_vectors, original_image_size = preprocess(data_path, target_resolution, target_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788acd3e-e391-4dfe-a5c3-4922feab0b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_vectors, original_imsize = preprocess(data_path, target_resolution, target_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "a3e93f27-2536-46ec-bcf0-50213e07a053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.14285505 1.14285505]\n",
      " [0.99999817 0.99999817]]\n",
      "[[216 256]\n",
      " [232 256]]\n"
     ]
    }
   ],
   "source": [
    "print(scale_vectors)\n",
    "\n",
    "print(original_image_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a240090-a4ba-43d3-838d-78462f1f2e9a",
   "metadata": {},
   "source": [
    "#### Denoise the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc831d0-c58b-492d-bac7-dbdf0d0f6965",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess(data_path, target_resolution, target_size, denoise=True, alphaTV=0.2)\n",
    "# alpha = 0.1 geeft wel prima maar nog veel details, 0.3 is misschien net hoog."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee7ebf5-759d-4413-84cb-f79a64611111",
   "metadata": {},
   "source": [
    "### Back to original format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "18d164df-3296-4013-b53d-8b362c0c9e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtoformat(scale_vectors, original_image_size, mask_folder):\n",
    "    '''\n",
    "    scale_vectors: the vectors that were used in preprocessing to reach desired\n",
    "        resolution\n",
    "    original_image_size: the orginal sizes of the images\n",
    "    mask_folder: the folder where the masks (output from network) are located\n",
    "    '''\n",
    "    \n",
    "    foldername = 'finalmasks'\n",
    "    prev_file = '00000000000'  # so that the first new_patient is always true\n",
    "    i = -1\n",
    "    new_patient = True\n",
    "\n",
    "    # make folder if it doesn't exist yet\n",
    "    if not os.path.exists(foldername):\n",
    "        os.mkdir(foldername)\n",
    "    else:\n",
    "        print(foldername+' folder already exists. Continuing regardless.')\n",
    "    \n",
    "    # remove automatically made files\n",
    "    if os.path.exists(os.path.join(mask_folder, '.ipynb_checkpoints')):\n",
    "        os.rmdir(os.path.join(mask_folder, '.ipynb_checkpoints'))\n",
    "    \n",
    "    # only write files if the folder is empty\n",
    "    if len(os.listdir(foldername)) == 0:\n",
    "        # loop over masks\n",
    "        for file in sorted(os.listdir(mask_folder)):\n",
    "            \n",
    "            # update iterator if we go to the next patient\n",
    "            if prev_file[:11] != file[:11]:\n",
    "                i += 1\n",
    "                new_patient = True\n",
    "            else:\n",
    "                new_patient = False\n",
    "\n",
    "            px, py = scale_vectors[i][0], scale_vectors[i][1]\n",
    "            nx, ny = original_image_size[i][0], original_image_size[i][1]\n",
    "\n",
    "            file_path = os.path.join(mask_folder, file)\n",
    "            \n",
    "            mask = Image.open(file_path).convert('L')\n",
    "            mask = np.array(mask, dtype=np.uint8)\n",
    "            \n",
    "            scale_vector = [1/px, 1/py]\n",
    "            \n",
    "            # scale back\n",
    "            mask = transform.rescale(mask,\n",
    "                                     scale_vector,\n",
    "                                     order=0,\n",
    "                                     preserve_range=True,\n",
    "                                     mode='constant')\n",
    "            \n",
    "            # pad or crop back\n",
    "            mask = crop_pad_resize(mask, nx, ny)\n",
    "            \n",
    "            if new_patient:\n",
    "                if i > 0:\n",
    "                    # save the previous 3D np array (if there is one)\n",
    "                    niftimage = nib.Nifti1Image(threedimage, affine=np.eye(4))\n",
    "                    nib.save(niftimage, os.path.join('finalmasks', prev_file[:-17]+'_gt'+'.nii.gz'))\n",
    "                # if we have a new page, make new np array for new nii.gz file\n",
    "                threedimage = mask.reshape(nx, ny, 1)\n",
    "\n",
    "            else:\n",
    "                # add mask to 3D np array\n",
    "                mask_threed = mask.reshape(nx, ny, 1)\n",
    "                threedimage = np.concatenate([threedimage, mask_threed], 2)\n",
    "\n",
    "\n",
    "            # save file as PNG if wanted\n",
    "#             mask_loc = os.path.join(foldername, file)\n",
    "#             mask_fin = Image.fromarray(np.uint8(mask), mode=\"L\")\n",
    "            \n",
    "#             mask_fin.save(mask_loc, format='PNG')\n",
    "            prev_file = file\n",
    "        \n",
    "        # save last 3D np array\n",
    "        niftimage = nib.Nifti1Image(threedimage, affine=np.eye(4))\n",
    "        nib.save(niftimage, os.path.join('finalmasks', prev_file[:-17]+'_gt'+'.nii.gz'))\n",
    "    else:\n",
    "        print(foldername+' folder was not empty. No files were written.')\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "096ca421-baa3-4ffd-b15d-c1d0f6b62c79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mask_folder = 'dummy_masks'\n",
    "\n",
    "backtoformat(scale_vectors, original_image_size, mask_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defa40b5-ba04-493e-a527-1ea4b41e50f8",
   "metadata": {},
   "source": [
    "#### Test if saving as nii.gz went well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "cf8b5e9e-024b-4f90-8262-1f06d6777193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(216, 256, 10)"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = 'finalmasks/patient001_frame01_gt.nii.gz'\n",
    "img_nii = nib.load(file)\n",
    "img = img_nii.get_fdata()\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb914f9-6917-4297-a552-c4a8e121a387",
   "metadata": {},
   "source": [
    "### Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "7233c008-112e-4d1d-ac9c-dbf59590398e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finalmasks/patient001_frame01_gt.nii.gz\n",
      "finalmasks/patient001_frame01_gt.nii.gz\n",
      "finalmasks/patient001_frame01_gt.nii.gz\n",
      "finalmasks/patient001_frame01_gt.nii.gz\n",
      "finalmasks/patient001_frame01_gt.nii.gz\n",
      "finalmasks/patient001_frame01_gt.nii.gz\n",
      "finalmasks/patient001_frame01_gt.nii.gz\n",
      "finalmasks/patient001_frame01_gt.nii.gz\n",
      "finalmasks/patient001_frame01_gt.nii.gz\n",
      "finalmasks/patient001_frame01_gt.nii.gz\n"
     ]
    }
   ],
   "source": [
    "for file in sorted(os.listdir('dummy_masks')):\n",
    "    print(os.path.join('finalmasks', file[:-17]+'_gt'+'.nii.gz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "208ce3ac-32e0-4520-be0c-5b5988909993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(216, 256, 1)\n",
      "(216, 256, 1)\n",
      "(216, 256, 2)\n",
      "(216, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "nx, ny = (216, 256)\n",
    "threedimage = np.zeros((nx, ny, 1), int)\n",
    "print(threedimage.shape)\n",
    "mask = np.ones(216*256)\n",
    "mask_threed = mask.reshape(nx, ny, 1)\n",
    "print(mask_threed.shape)\n",
    "\n",
    "threedimage = np.concatenate([threedimage, mask_threed], 2)\n",
    "print(threedimage.shape)\n",
    "\n",
    "again = np.concatenate([threedimage, mask_threed], 2)\n",
    "print(again.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "b200aeeb-9faa-4c9d-9624-e5603a57de8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 4, 3)\n",
      "(3, 4, 2)\n"
     ]
    }
   ],
   "source": [
    "a1 = np.zeros((3, 4, 1), dtype=np.uint8)\n",
    "\n",
    "a2 = np.full((3, 4, 1), 2, dtype=np.uint8)\n",
    "\n",
    "x = np.concatenate([a1, a2], 2)\n",
    "\n",
    "a3 = np.full((3, 4, 1), 3, dtype=np.uint8)\n",
    "\n",
    "y = np.concatenate([x, a3], 2)\n",
    "print(y.shape)\n",
    "\n",
    "y = np.delete(y, 0, axis=2)\n",
    "print(y.shape)\n",
    "\n",
    "# x = np.arange(4*4*3).reshape(4,4,3)\n",
    "# print(x.shape)\n",
    "# ni_img = nib.Nifti1Image(x, affine=np.eye(4))\n",
    "# niftimage = nib.Nifti1Image(y, affine=np.eye(4))\n",
    "\n",
    "# nib.save(niftimage, os.path.join('finalmasks', 'test3d.nii.gz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "97975c18-4f04-4acf-88c8-7687ee78bb84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 2)\n",
      "(4, 2)\n",
      "(4, 1)\n"
     ]
    }
   ],
   "source": [
    "y = [[0, 0]]\n",
    "y = np.append(scale_vectors, [[1.14285505, 1.14285505]], axis=0)\n",
    "x = [[0, 0]]\n",
    "x = np.append(original_image_size, [[216, 256]], axis=0)\n",
    "\n",
    "print(y.shape)\n",
    "print(x.shape)\n",
    "\n",
    "y = np.delete(y, 0, axis=1)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "46c5fb98-40de-4269-8171-b6c41f72e076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1428550530650459\n",
      "1.1428550530650459\n"
     ]
    }
   ],
   "source": [
    "x, y = scale_vectors[0][0], scale_vectors[0][1]\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "15743791-071b-42b7-9144-19c8d21c1bdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.14285505, 1.14285505],\n",
       "       [0.99999817, 0.99999817]])"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scale_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd93a2c3-3209-4301-9d72-e1c6666c479a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
